{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "\n",
        "\n",
        "گراف محاسباتی گراف‌هایی جهت دار هستند که برای انجام عملیات‌های ریاضی تشکیل می‌شوند و در واقع عملیات ‌های ریاضی در آن شکسته می‌شود و برای محاسبات backward و forward در شبکه استفاده می‌شوند.\n",
        "برای تشکیل گراف محاسباتی از کلاس Tensor استفاده می‌کنیم به طوری که کوچکترین ساختار موجود در پیاده سازی MLP تنسورها هستند و وزن‌ها و بایاس‌ها و ورودی‌های را از نوع تنسور تعریف می‌کنیم.\n",
        "\n",
        "1. ایجاد تانسورها:\n",
        "   - هر تانسور شامل این بخش‌ها است: مقدار (value)،  و اطلاعات مربوط به ورودی‌ها  (children ).\n",
        "   - Value مقدار عددی Tensor.\n",
        "  \n",
        "   - children یک مجموعه از تنسور های فرزند، که معمولاً در عملیات ریاضی مورد استفاده قرار می‌گیرند که به عنوان ورودی به این تانسور داده می‌شوند. به این ترتیب، تانسورها می‌توانند به صورت سلسله‌مراتبی متصل شوند تا به یک گراف محاسباتی منتهی شوند.\n",
        "   - با استفاده از magic methods عملگرهای ضرب و جمع و توان و تفریق را باز تعریف می‌کنیم به طوری که اعمال آنها بر روی تنسور معنا دار شود.\n",
        "2. ایجاد گراف محاسباتی:\n",
        "   - تنسورها به یکدیگر متصل می‌شوند و یک گراف محاسباتی را تشکیل می‌دهند.\n",
        "   - هر اتصال بین تانسورها نمایش‌دهنده یک عملگر ریاضی است که ورودی‌ها را به خروجی مرتبط می‌کند.\n",
        "   - این گراف محاسباتی نشان‌دهنده ساختار محاسباتی مورد نظر است و نحوه ترکیب و ارتباط بین تانسورها را مشخص می‌کند.\n",
        "3. محاسبه مقدار خروجی:\n",
        "   - پس از ایجاد گراف محاسباتی، می‌توان مقدار خروجی را محاسبه کرد.\n",
        "   - این کار با اجرای عملیات ریاضی بر روی مقادیر ورودی‌ها و انتشار آن در گراف محاسباتی انجام می‌شود.\n",
        "   - عملیات انتشار از ورودی‌ها به سمت خروجی، با استفاده از قوانین محاسبات ریاضی انجام می‌گیرد.\n",
        "   - در هر گره گراف، عملیات مربوط به عملگر آن گره اجرا می‌شود و نتیجه به گره بعدی ارسال می‌گردد.\n",
        "4. محاسبه مشتق‌ها (Backpropagation):\n",
        "   - برای محاسبه مشتق‌ها، تابع backward() فراخوانی می‌شود.\n",
        "   - ابتدا یک مرتب‌سازی توپولوژیکی از تانسورها انجام می‌شود تا ترتیب صحیح محاسبات مشخص شود.(در این مرحله، با استفاده از یک الگوریتم مرتب‌سازی توپولوژیکی، تنسورها به ترتیبی مرتب می‌شوند که هر تنسور قبل از تنسورهای وابسته به آن محاسبه شود.)\n",
        "   - سپس از آخرین تانسور (خروجی) شروع کرده و به صورت معکوس (Reverse Mode) روی گراف محاسباتی حرکت می‌کند.\n",
        "   - در این مرحله، قوانین زنجیره‌ای برای محاسبه مشتق‌ها به کار گرفته می‌شود.\n",
        "5. محاسبه گرادیان در هر تانسور:\n",
        "   - در هر تانسور، تابع _backward() فراخوانی می‌شود.\n",
        "   - این تابع با استفاده از قوانین زنجیره‌ای، مشتق مقدار تانسور را نسبت به ورودی‌های خود محاسبه می‌کند.\n",
        "   - نتیجه این محاسبات در ویژگی grad هر تانسور ذخیره می‌شود.\n",
        "6. انتشار گرادیان‌ها در گراف:\n",
        "   - به این ترتیب، گرادیان‌ها به صورت معکوس در گراف محاسباتی منتشر می‌شوند.\n",
        "   - این انتشار گرادیان‌ها، مقادیر مشتق را نسبت به تمام پارامترهای شبکه (وزن‌ها و بایاس‌ها) محاسبه می‌کند.\n",
        "   - این الگوریتم پس‌انتشار (Backpropagation)، امکان محاسبه مشتق‌گیری اتوماتیک را فراهم می‌کند.\n",
        "   - این روش به طور گسترده در آموزش مدل‌های مبتنی بر گرادیان مانند شبکه‌های عصبی مصنوعی استفاده می‌شود.\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "wt0rVPKVsFXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tensor:\n",
        "\n",
        "  def __init__(self, value, children=()):\n",
        "    self.value = value\n",
        "    self.children = set(children)\n",
        "    self.grad = 0\n",
        "    self._backward = lambda  : None\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return f\"Tensor(value={self.value})\"\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "\n",
        "    out = Tensor(self.value*other.value, children= (self, other))\n",
        "\n",
        "    def backward():\n",
        "      self.grad += other.value * out.grad\n",
        "      other.grad += self.value * out.grad\n",
        "\n",
        "    out._backward = backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __add__(self, other):\n",
        "\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "\n",
        "    out = Tensor(self.value+other.value, children= (self, other))\n",
        "\n",
        "    def backward():\n",
        "      self.grad += 1 * out.grad\n",
        "      other.grad += 1 * out.grad\n",
        "\n",
        "    out._backward = backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "\n",
        "  def __pow__(self, other):\n",
        "\n",
        "    input_value = self.value\n",
        "    output_value = pow(input_value, other)\n",
        "\n",
        "    out = Tensor(output_value, children=(self, ))\n",
        "\n",
        "    def backward():\n",
        "      self.grad += other * pow(self.value, other-1) * out.grad\n",
        "\n",
        "    out._backward = backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "\n",
        "    out = Tensor(self.value-other.value, children= (self, other))\n",
        "\n",
        "    def backward():\n",
        "      self.grad += 1 * out.grad\n",
        "      other.grad += (-1) * out.grad\n",
        "\n",
        "    out._backward = backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "\n",
        "    topo_sort = []\n",
        "    visited = set()\n",
        "\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v.children:\n",
        "          build_topo(child)\n",
        "        topo_sort.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1\n",
        "\n",
        "    for node in reversed(topo_sort):\n",
        "      node._backward()"
      ],
      "metadata": {
        "id": "toIDtdxz4nnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "در کلاس F دو متد استاتیک برای توابع فعالساز تعریف می‌کنیم به طوری که تابع tanh یک تنسور می‌گیرد و یک تنسور که برابر است با مقدارtanh(x) است برمی‌گرداند که فرزند آن تنسور ورودی است. برای تابع backward هم مقدار گرادیان فرزند برابر است با مشتق tanh ضربدر گرادیان tanh\n",
        "\n",
        "تابع sign هم پیاده سازی تابع علامت است که برای اعداد نامنفی تنسور ۱ و برای اعداد منفی تنسور -۱ را برمی‌گرداند.\n",
        "\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "ijrYavK6CSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class F:\n",
        "  @staticmethod\n",
        "\n",
        "  def tanh(X: Tensor) -> Tensor:\n",
        "    input_value = X.value\n",
        "    output_value = (math.exp(input_value) - math.exp(-input_value))/ (math.exp(input_value) + math.exp(-input_value))\n",
        "\n",
        "    out = Tensor(output_value, children=(X, ))\n",
        "\n",
        "    def backward():\n",
        "      X.grad += (1-(output_value**2))*out.grad\n",
        "\n",
        "    out._backward = backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def sign(x):\n",
        "    if x.value >= 0:\n",
        "      return Tensor(1)\n",
        "    elif x.value < 0:\n",
        "      return Tensor(-1)"
      ],
      "metadata": {
        "id": "BhfgdY4vlEhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "کلاس نورون در واقع کلاسی است که در آن مقدار ورودی‌های وارد به نورون در وزن متناظر انها ضرب می‌شود و با بایاس جمع می‌شود.\n",
        "برای ساخت instance از کلاس neuron پارامتر اندازه ورودی گرفته می‌شود و در ابتدا وزن‌ها تنسوری های رندومی با مقادیر بین -۱ و ۱ است که اندازه لیست وزن‌ها برابر است با همان اندازه ورودی و بایاس هم عددی رندوم بین منفی یک و یک است.\n",
        "\n",
        "متد forward برای ورودی x و وزن‌ها مقدار w_i * x_i متناظر را حساب می‌کند و جمع می‌بندد و تابع فعالساز را بر روی آن اعمال می‌کند و سپس برمی‌گرداند.\n",
        "\n",
        "متد call هم صرفا متد forward را برای ورودی x صدا می‌زند\n",
        "\n",
        "متد parameters در لیستی مقدار وزن‌ها و بایاس را برمی‌گرداند.\n",
        "\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "WiTuEeHoNVNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "  def __init__(self, input_size):\n",
        "\n",
        "    self.weights = [Tensor(random.uniform(-1, 1)) for i in range(input_size)]\n",
        "    self.bias = Tensor(random.uniform(-1, 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = sum([w_i * x_i for w_i, x_i in zip(self.weights, x)])\n",
        "    return F.tanh(res + self.bias)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.weights + [self.bias]\n"
      ],
      "metadata": {
        "id": "YzuqDAR-DOTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "کلاس Layer در واقع مجموعه ای از نمونه‌های کلاس Neuron است که لایه‌های شبکه   هستند و هر لایه باید بداند که تعداد نورون‌های ورودی چند تاست و به چه تعداد نورون باید تبدیل شود به همین دلیل در ساخت شی از پارامتر های input_size و output_size استفاده می‌کنیم و لیستی از نورون‌ها به تعداد سایز لایه خروجی با نورون‌هایی با سایز لایه ورودی را به عنوان اتریبیوت برای این کلاس در نظر می‌گیریم.\n",
        "\n",
        "متد forward برای هر نورون لایه متد forward کلاس Neuron را کال می‌کند و به این شکل خروجی لایه را با اعمال بر روی هر نورون ورودی انجام  می‌دهد\n",
        "\n",
        "متد call متد forward را صدا می‌زند برای ورودی x\n",
        "\n",
        "متد parameters هم پارامترهای هر نورون که شامل وزن‌ها و بایاس‌ها است را بر می‌گرداند\n",
        "\n",
        "\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "z_LjwX3hJX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.neurons = [Neuron(input_size) for _ in range(output_size)]\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = [neuron(x) for neuron in self.neurons]\n",
        "    return out[0] if len(out)==1 else out\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def parameters(self):\n",
        "    return [param for neuron in self.neurons for param in neuron.parameters()]\n",
        "\n"
      ],
      "metadata": {
        "id": "PwsYPg3EDSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "کلاس MLP در واقع جایی است که شبکه تشکیل می‌شود به طوری که اندازه ورودی(تعداد فیچرها)و سایز لایه‌ها را می‌گیرد. لیست layers_total شامل تعداد نورون‌های هر لایه‌است که از کنار هم گذاشتن تعداد فیچرها و لیست تعداد نورون‌های هر لایه به وجود آمده است.\n",
        "\n",
        "همانطور که گفته شد اشیای کلاس Layer با استفاده از تعداد نورون‌های ورودی و خروجی لایه ساخته می‌شوند به همین ترتیب لیستی از لایه‌ها را تشکیل می‌دهیم\n",
        "\n",
        "متد forward برای هر لایه شبکه متد call لایه را صدا می‌زند و آن را بر می‌گرداند\n",
        "\n",
        "و متد call هم متد forward را صدا می‌زند\n",
        "\n",
        "متد parameters پارامترهای هر لایه را صدا می‌زند با استفاده از متد parametes در لایه ها.\n",
        "\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "tzy9pXbIzsVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__(self, input_size, layer_sizes):\n",
        "    layers_total = [input_size] + layer_sizes\n",
        "    self.layers = [Layer(layers_total[i], layers_total[i+1]) for i in range(len(layer_sizes))]\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def parameters(self):\n",
        "    return [param for layer in self.layers for param in layer.parameters()]"
      ],
      "metadata": {
        "id": "BHjvREvSDilR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "تابع criterion برای محاسبه خطا استفاده می‌شود به طوری که با گرفتن لیبل‌های\n",
        "پیش بینی شده توسط شبکه و لیبل‌های درست جمع توان دو اختلاف لیبل‌ها را محاسبه می‌کند و در نهایت این مقدار بر تعداد رکوردها تقسیم می‌کند و برمی‌گرداند در واقع هدف شبکه کاهش مقدار همین تابع است.\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "FJAbx4lrOY3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(y_hats: list[Tensor], Y) -> Tensor:\n",
        "    loss = Tensor(0)\n",
        "    for y_hat, y in zip(y_hats, Y):\n",
        "        loss += pow(y_hat - y, 2)\n",
        "    return pow(Tensor(len(y_hats)),-1)*loss"
      ],
      "metadata": {
        "id": "Pnaj2AitPnE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "کلاس optimizer برای ساخت نمونه، پارامترهای شبکه و نرخ یادگیری را می‌گیرد و به عنوان اتریبیوت ذخیره می‌کند\n",
        "با استفاده از متد zero_grad در هر epoch مقدار گرادیان پارامترها را صفر می‌کنیم\n",
        "\n",
        "با استفاده از متد step مقادیر را آپدیت میکنیم به طوری که مقدار جدید برابر است با مقدار فعلی منهای (گرادیان*نرخ یادگیری).\n",
        "\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "IWBffsI7P7vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, parameters, lr):\n",
        "        self.parameters = parameters\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.parameters:\n",
        "            param.grad = 0\n",
        "\n",
        "    def step(self):\n",
        "        for param in self.parameters:\n",
        "            param.value -= self.lr * param.grad"
      ],
      "metadata": {
        "id": "nhUdG0tclt-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aJbulxjIv72z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "برای تست شبکه از مجموعه داده make_blobs کتابخانه sklearn استفاده می‌کنیم که دارای ۲۰ نمونه و ۱۵ فیچر است.لیبل‌های این مجموعه داده صفر یا یک هستند برای ایجاد تناسب با تابع فعالساز tanh لیبل های صفر را منفی یک تبدیل می‌کنیم.\n",
        "\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "xK9w8EhvQxae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, Y = make_blobs(n_samples=10, n_features=10, centers=2, random_state=41)\n",
        "Y = [Tensor(-1) if i == 0 else Tensor(1) for i in Y]"
      ],
      "metadata": {
        "id": "2-1UHLBhts3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "برای ساخت نمونه از مدل نیاز به اندازه ورودی داریم که همان تعداد فیچرهاست.و اندازه لایه‌های مخفی را به ترتیب ۲،۳ و لایه خروجی را ۱ نورون در نظر می‌گیریم و مدل را می‌سازیم\n",
        "پارامترهای مدل ساخته شده و نرخ یادگیری را به عنوان ورودی برای ساخت شی optimizer استفاده می‌کنیم.\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "TCluMgkiRl5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X.shape[1]\n",
        "layer_sizes = [2,3,1]\n",
        "model = MLP(input_size, layer_sizes)\n",
        "optimizer = Optimizer(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "SBMoOK_JAiEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"B Mitra\" size=4>\n",
        "<div dir=rtl>\n",
        "<p>\n",
        "در ادامه برای تعداد epoch = 20 y_hat را برای رکوردهای موجود در مجموعه داده بدست می‌آوریم و بر اساس تابع criterion مقدار loss که یک تنسور است بدست می‌آید سپس با استفاده از متد zero_grad گرادیان ها را صفر می‌کنیم و عملیات backward را برای loss شروع می‌کنیم و به همین ترتیب گرادیان ها مطابق بخش Tensor آپدیت می‌شوند سپس مقادیر بایاس و وزن ها را برحسب مشتقات آپدیت می‌کنیم\n",
        "\n",
        "خروجی y_hats بین یک و منفی یک است با استفاده از تابع sign برای مقادیر نامنفی لیبل یک را اختصاص می‌دهیم و برای مقادیر منفی لیبل منفی یک را\n",
        "</div>\n",
        "</font>"
      ],
      "metadata": {
        "id": "5KaGZgaOTYE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 200\n",
        "for _ in range(n_epochs):\n",
        "  sample = [x for x in X]\n",
        "  y_hats = [model(x) for x in X]\n",
        "  loss = criterion(y_hats, Y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "y_hats = [F.sign(i) for i in y_hats]\n",
        "print((y_hats))\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "Z-w0Dk0uAkJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d196953-7e04-44e5-b798-cfcb7cf3d0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tensor(value=1), Tensor(value=1), Tensor(value=1), Tensor(value=1), Tensor(value=-1), Tensor(value=-1), Tensor(value=-1), Tensor(value=1), Tensor(value=-1), Tensor(value=-1)]\n",
            "[Tensor(value=1), Tensor(value=1), Tensor(value=1), Tensor(value=1), Tensor(value=-1), Tensor(value=-1), Tensor(value=-1), Tensor(value=1), Tensor(value=-1), Tensor(value=-1)]\n"
          ]
        }
      ]
    }
  ]
}